To begin this ETL data processor  project our first objective was to find the two datasets to use. The first one we chose was an NBA CSV file from 2012 on game data and the second one is the CoinCap API which provides current prices and activity of cryptocurrencies in JSON format.  
We started off by organising the data in the NBA dataset which presented a few initial challenges. We focused on organising the NBA dataset in order to filter out games with points per game below a specified threshold. We also calculated an efficiency rating after finding the equation for it in an article and sorted the data by its average. These processes came with some difficulties regarding iterating through pandas data frame rows and making changes to them. This was easily solved however by looking into some different approaches we could use with pandas.   
	Once the NBA dataframe had been modified and stored, we then moved on to working with the crypto API. The first issue we ran into was that we had initially chosen a separate API, however it required authorization and we figured it would be easier to find an API with no authorization for it to be more accessible. We tried a couple of different APIâ€™s until we found this one that was easy to integrate into our project. The next issue we ran into was that we couldn't hit the rate limit when testing with the API. We managed to solve this by calling the API and using that data from the call to mock the API call and test on that data.   
	Since we were using the Pandas library package from python, converting from CSV to JSON and to other formats was easy since we converted them into pandas dataframes for the input. Error handling was also easier than expected and we checked to make sure that the processors outputted an informative message when an input did not work.   
	The ETL processor we have developed is a tool that can be utilised to streamline data integration for various data projects. Its adaptability makes it a versatile tool since it can adapt to various data formats such as JSON and CSV. The standardised data processing allows scalability in the sense that one can easily integrate new data formats to accommodate different projects. Furthermore, the error handling ensures  processing reliability. The processor we created can be used for a variety of projects such as in sports betting analytics, and even market research in either field. 

